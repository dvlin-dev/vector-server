import { Inject, Injectable, LoggerService, forwardRef } from '@nestjs/common'
import { CreateConversationDto } from './dto/create-conversation.dto'
import { PrismaService } from 'src/utils/prisma/prisma.service'
import { getKeyConfigurationFromEnvironment } from 'src/utils/llm/configuration'
import { CompletionsDto, CompletionsRegularDto } from './dto/chat.dto'
import { ConfigService } from '@nestjs/config'
import OpenAI from 'openai'
import { ModelType } from 'src/types/chat'
import { CreateMessageDto } from './dto/create-message.dto'
import { MessageSummaryService } from './message-summary.service'

import { Response } from 'express'
import { Readable } from 'stream'
import { WINSTON_MODULE_NEST_PROVIDER } from 'nest-winston'
import { VectorService } from '../vector/vector.service'
import { systemPrompt } from 'src/utils/llm/prompt'

@Injectable()
export class ConversationService {
  private openai: OpenAI

  constructor(
    private prisma: PrismaService,
    private configService: ConfigService,
    private messageSummaryService: MessageSummaryService,
    @Inject(forwardRef(() => VectorService))
    private vectorService: VectorService,
    @Inject(WINSTON_MODULE_NEST_PROVIDER)
    private readonly logger: LoggerService
  ) {
    const keyConfiguration = getKeyConfigurationFromEnvironment(this.configService)

    // åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
    if (keyConfiguration.apiType === ModelType.AZURE_OPENAI) {
      this.openai = new OpenAI({
        apiKey: keyConfiguration.azureApiKey,
        baseURL: `https://${keyConfiguration.azureInstanceName}.openai.azure.com/openai/deployments/${keyConfiguration.azureDeploymentName}`,
        defaultQuery: { 'api-version': keyConfiguration.azureApiVersion },
        defaultHeaders: {
          'api-key': keyConfiguration.azureApiKey,
        },
      })
    } else {
      // ç¡®ä¿ apiKey å­˜åœ¨
      if (!keyConfiguration.apiKey) {
        console.error(
          'Missing OpenAI API key, please check the environment variable OPENAI_API_KEY'
        )
      }

      this.openai = new OpenAI({
        apiKey: keyConfiguration.apiKey,
        baseURL: keyConfiguration.basePath,
      })
    }

    // éªŒè¯ API å¯†é’¥æ˜¯å¦è®¾ç½®æ­£ç¡®
    console.log('OpenAI configuration:', {
      apiType: keyConfiguration.apiType,
      hasApiKey: !!keyConfiguration.apiKey,
      apiModel: keyConfiguration.apiModel,
      basePath: keyConfiguration.basePath,
    })
  }

  get(id: string) {
    return this.prisma.conversation.findUnique({
      where: {
        id,
      },
      include: {
        messages: true,
      },
    })
  }

  getAll() {
    return this.prisma.conversation.findMany()
  }

  create(createConversationDto: CreateConversationDto) {
    const { abstract, siteId } = createConversationDto
    return this.prisma.conversation.create({
      data: {
        abstract: abstract || '',
        siteId,
      },
    })
  }

  delete(id: string) {
    return this.prisma.conversation.delete({
      where: {
        id,
      },
    })
  }

  /**
   * é€šç”¨çš„ AI è¯·æ±‚æ–¹æ³•ï¼Œç›´æ¥é€ä¼  OpenAI çš„ CreateChatCompletionRequest æ ¼å¼
   * @param request OpenAI çš„åŸç”Ÿè¯·æ±‚æ ¼å¼
   * @returns AI çš„å›å¤å†…å®¹
   */
  async completions(request: CompletionsRegularDto): Promise<string> {
    try {
      const { messages, model, temperature } = request
      const systemMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
        role: 'system',
        content: systemPrompt,
      }
      const openaiMessages = [
        systemMessage,
        ...messages,
      ]

      const response = await this.openai.chat.completions.create({
        model: model || this.configService.get('OPENAI_API_MODEL') || 'gpt-4o',
        messages: openaiMessages, 
        temperature,
      })

      return response.choices[0].message.content
    } catch (error) {
      console.error('OpenAI API error:', {
        message: error.message,
        status: error.response?.status,
        data: error.response?.data,
      })

      throw new Error(`Failed to call OpenAI API: ${error.message}`)
    }
  }

  async completionsStream(completionsDto: CompletionsDto, res: Response) {
    const { messages, conversationId, siteId } = completionsDto

    // 1. ä¿å­˜ç”¨æˆ·å‘é€çš„æœ€åä¸€æ¡æ¶ˆæ¯åˆ°æ•°æ®åº“
    const lastUserMessage = messages[messages.length - 1]
    if (lastUserMessage.role === 'user') {
      await this.createMessage({
        conversationId,
        content: lastUserMessage.content,
        role: lastUserMessage.role,
      })
    }

    try {
      // 2. å¼‚æ­¥æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œæ¶ˆæ¯æ€»ç»“ï¼ˆä¸é˜»å¡å½“å‰è¯·æ±‚ï¼‰
      this.handleMessageSummarization(conversationId).catch(error => {
        this.logger.error('æ¶ˆæ¯æ€»ç»“å¤„ç†å¤±è´¥', error)
      })

      // 3. è·å–ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡æ¶ˆæ¯ï¼ˆä½¿ç”¨æ€»ç»“+æœ€è¿‘æ¶ˆæ¯ï¼‰
      const contextMessages = await this.messageSummaryService.getContextMessages(conversationId)

      const systemMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
        role: 'system',
        content: systemPrompt,
      }

      const referenceMessages = await this.getReferenceMessages(siteId, contextMessages)
      // ä½¿ç”¨ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯ä¼ å…¥çš„æ‰€æœ‰æ¶ˆæ¯
      const openaiMessages = [
        systemMessage,
        ...referenceMessages,
      ]

      const isGpt5 = this.configService.get('OPENAI_API_MODEL_2')?.includes('gpt-5');
      
      // å®šä¹‰ function tool
      const tools: OpenAI.Chat.Completions.ChatCompletionTool[] = [{
        type: 'function',
        function: {
          name: 'extract_unanswerable_question',
          description: 'å½“èƒŒæ™¯ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·é—®é¢˜æ—¶ï¼Œæå–ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜',
          parameters: {
            type: 'object',
            properties: {
              question: {
                type: 'string',
                description: 'ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜ï¼Œç²¾ç‚¼ä¸”æ¸…æ™°'
              },
              type: {
                type: 'string',
                description: 'ç”¨æˆ·çš„é—®é¢˜ç±»å‹ï¼Œä¾‹å¦‚ï¼šäº§å“å’¨è¯¢ã€å”®åæœåŠ¡ã€ä»·æ ¼å’¨è¯¢ã€å…¶ä»–ç­‰'
              }
            },
            required: ['question', 'type']
          }
        }
      }];
      
      // 4. ä½¿ç”¨ OpenAI API å‘é€æµå¼è¯·æ±‚
      const response = await this.openai.chat.completions.create({
        model: this.configService.get('OPENAI_API_MODEL_2') || 'gpt-4o',
        messages: openaiMessages,
        stream: true,
        tools: tools,
        ...(isGpt5 ? {
          verbosity: "low",
          reasoning_effort: "minimal"
        }:{})
      })

      // å­˜å‚¨å®Œæ•´çš„å“åº”
      let fullResponse = ''

      // å¤„ç†æµå¼å“åº” - æ–°ç‰ˆæœ¬ OpenAI SDK
      try {
        let toolCallId = null;
        let toolCallName = null;
        let toolCallArguments = '';
        let isCollectingToolCall = false;
        
        for await (const chunk of response) {
          // å¤„ç†å·¥å…·è°ƒç”¨å¼€å§‹
          if (chunk.choices[0]?.delta?.tool_calls?.[0]?.function) {
            const toolCall = chunk.choices[0].delta.tool_calls[0].function;
            
            // å¦‚æœæ˜¯å·¥å…·è°ƒç”¨çš„å¼€å§‹éƒ¨åˆ†
            if (toolCall.name) {
              isCollectingToolCall = true;
              toolCallId = chunk.choices[0].delta.tool_calls[0].index;
              toolCallName = toolCall.name;
              toolCallArguments = toolCall.arguments || '';
              continue;
            }
            
            // å¦‚æœæ˜¯å·¥å…·è°ƒç”¨å‚æ•°çš„ç»§ç»­éƒ¨åˆ†
            if (isCollectingToolCall && toolCall.arguments) {
              toolCallArguments += toolCall.arguments;
              continue;
            }
          }
          
          // å¤„ç†æ™®é€šå†…å®¹
          const content = chunk.choices[0]?.delta?.content || '';
          
          if (content) {
            // æ›´æ–°å®Œæ•´å“åº”
            fullResponse += content;

            // å‘é€åˆ°å®¢æˆ·ç«¯
            res.write(`data: ${JSON.stringify({ content })}\n\n`);
          }
        }
        
        // å¦‚æœæ”¶é›†åˆ°äº†å·¥å…·è°ƒç”¨ï¼Œå¤„ç†å®ƒ
        if (isCollectingToolCall && toolCallName === 'extract_unanswerable_question') {
          try {
            const toolCallData = JSON.parse(toolCallArguments);
            // å‘é€å·¥å…·è°ƒç”¨ç»“æœåˆ°å®¢æˆ·ç«¯
            res.write(`data: ${JSON.stringify({ 
              tool_call: {
                name: toolCallName,
                arguments: toolCallData
              }
            })}\n\n`);
            
          } catch (parseError) {
            console.error('è§£æå·¥å…·è°ƒç”¨å‚æ•°å¤±è´¥:', parseError);
          }
        }

        // å‘é€å®Œæˆæ ‡è®°
        res.write('data: [DONE]\n\n');
        res.end();

        // åœ¨æµç»“æŸåï¼Œå°†å®Œæ•´å“åº”ä¿å­˜åˆ°æ•°æ®åº“
        if (fullResponse.trim()) {
          await this.createMessage({
            conversationId,
            content: fullResponse,
            role: 'assistant',
          });
        }
      } catch (streamError) {
        console.error('Stream processing error:', streamError)
        res.write(`data: ${JSON.stringify({ error: 'system error' })}\n\n`)
        res.write('data: [DONE]\n\n')
        res.end()
      }
    } catch (error) {
      console.error('OpenAI API stream response error:', error)

      // å‘é€é”™è¯¯ä¿¡æ¯ç»™å®¢æˆ·ç«¯
      res.write(`data: ${JSON.stringify({ error: 'system error' })}\n\n`)
      res.write('data: [DONE]\n\n')
      res.end()

      throw error
    }
  }

  private async getReferenceMessages(siteId: string, contextMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[]): Promise<OpenAI.Chat.Completions.ChatCompletionMessageParam[]> {
    if (!siteId) {
      return contextMessages
    }

    const lastestUserMessage = contextMessages[contextMessages.length - 1];
    const remainingMessages = contextMessages.slice(0, -1);

    const reference = await this.vectorService.similaritySearch({
      message: lastestUserMessage.content as string,
      size: 1,
      siteId,
    })

    const referenceContent = `
<å‚è€ƒèƒŒæ™¯>
${reference.map(item => item.pageContent).join('\n')}
</å‚è€ƒèƒŒæ™¯>

<ç”¨æˆ·é—®é¢˜>
${lastestUserMessage.content}
</ç”¨æˆ·é—®é¢˜>
`
    const addReferenceUserMessages = {
      role: 'user',
      content: referenceContent,
    } as OpenAI.Chat.Completions.ChatCompletionUserMessageParam
    
    return [...remainingMessages, addReferenceUserMessages]
  }

  /**
   * å¤„ç†æ¶ˆæ¯æ€»ç»“é€»è¾‘ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰
   */
  private async handleMessageSummarization(conversationId: string): Promise<void> {
    try {
      // æ£€æŸ¥æ˜¯å¦éœ€è¦æ€»ç»“
      const shouldSummarize = await this.messageSummaryService.shouldSummarizeMessages(conversationId)
      
      if (shouldSummarize) {
        this.logger.log(`å¼€å§‹å¯¹è¯æ€»ç»“ï¼Œå¯¹è¯ID: ${conversationId}`)
        await this.messageSummaryService.summarizeMessages(conversationId)
        this.logger.log(`å¯¹è¯æ€»ç»“å®Œæˆï¼Œå¯¹è¯ID: ${conversationId}`)
      }
    } catch (error) {
      this.logger.error(`æ¶ˆæ¯æ€»ç»“å¤„ç†å¤±è´¥ï¼Œå¯¹è¯ID: ${conversationId}`, error)
    }
  }

  // æ¶ˆæ¯ç›¸å…³æ–¹æ³•
  getMessage(id: string) {
    return this.prisma.message.findUnique({
      where: {
        id,
      },
    })
  }

  /**
   * è·å–å¯¹è¯æ¶ˆæ¯ï¼ˆæ™ºèƒ½è¿”å›ä¼˜åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨ï¼‰
   * å½“å­˜åœ¨æ€»ç»“æ—¶ï¼Œåªæ˜¾ç¤ºæœ€æ–°æ€»ç»“åçš„æ¶ˆæ¯ï¼Œé¿å…é‡å¤æ˜¾ç¤º
   * å¯¹å‰ç«¯å®Œå…¨é€æ˜
   */
  async getMessages(conversationId: string) {
    const lastSummary = await this.messageSummaryService.getLastSummaryMessage(conversationId)
    
    if (lastSummary) {
      // å¦‚æœæœ‰æ€»ç»“ï¼Œè¿”å›æ€»ç»“ + æ€»ç»“åçš„æ¶ˆæ¯
      const messagesAfterSummary = await this.prisma.message.findMany({
        where: {
          conversationId,
          createdAt: { gt: lastSummary.createdAt },
        },
        orderBy: {
          createdAt: 'asc',
        },
        select: {
          id: true,
          content: true,
          role: true,
          createdAt: true,
        },
      })

      // ä¸ºæ€»ç»“æ¶ˆæ¯æ·»åŠ ç‰¹æ®Šæ ‡è¯†ï¼Œå‰ç«¯å¯ä»¥æ®æ­¤è¿›è¡Œç‰¹æ®Šæ ·å¼å¤„ç†
      return [
        {
          id: lastSummary.id,
          content: `ğŸ“‹ [å¯¹è¯æ€»ç»“ - åŒ…å«${lastSummary.originalCount}æ¡å†å²æ¶ˆæ¯]\n\n${lastSummary.content}`,
          role: lastSummary.role,
          createdAt: lastSummary.createdAt,
        },
        ...messagesAfterSummary,
      ]
    } else {
      // å¦‚æœæ²¡æœ‰æ€»ç»“ï¼Œè¿”å›æ‰€æœ‰éæ€»ç»“æ¶ˆæ¯
      return this.prisma.message.findMany({
        where: {
          conversationId,
        },
        orderBy: {
          createdAt: 'asc',
        },
        select: {
          id: true,
          content: true,
          role: true,
          createdAt: true,
        },
      })
    }
  }

  /**
   * è·å–å¯¹è¯çš„æ‰€æœ‰åŸå§‹æ¶ˆæ¯ï¼ˆåŒ…æ‹¬æ€»ç»“æ¶ˆæ¯ï¼Œç”¨äºç®¡ç†å’Œè°ƒè¯•ï¼‰
   */
  getAllRawMessages(conversationId: string) {
    return this.prisma.message.findMany({
      where: {
        conversationId,
      },
      orderBy: {
        createdAt: 'asc',
      },
    })
  }

  async createMessage(createMessageDto: CreateMessageDto) {
    const { conversationId, content, role } = createMessageDto

    // åˆ›å»ºæ¶ˆæ¯
    const message = await this.prisma.message.create({
      data: {
        conversationId,
        content,
        role,
        isSummary: false,
        createdAt: new Date(),
      },
    })

    return message
  }

  deleteMessage(id: string) {
    return this.prisma.message.delete({
      where: {
        id,
      },
    })
  }

}
